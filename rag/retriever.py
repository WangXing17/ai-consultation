"""
æ£€ç´¢å™¨æ¨¡å—
å®ç°å¤šè·¯å¬å›ï¼ˆè¯­ä¹‰å‘é‡æ£€ç´¢ + å…³é”®è¯/å€’æ’æ£€ç´¢ + è§„åˆ™å¬å›ï¼‰ä¸é‡æ’ç­–ç•¥
"""
import jieba
import re
from multiprocessing import Pool, cpu_count
from typing import List, Dict, Any, Tuple
from rank_bm25 import BM25Okapi
from pymilvus import Collection, connections
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from config import settings
from models import KnowledgeSource


def _jieba_tokenize_one(text: str) -> List[str]:
    """å•æ¡æ–‡æœ¬åˆ†è¯ï¼Œä¾›å¤šè¿›ç¨‹ Pool.map ä½¿ç”¨ï¼ˆé¡»ä¸ºæ¨¡å—çº§å‡½æ•°ä»¥æ”¯æŒ pickleï¼‰"""
    return list(jieba.cut(text or ""))


class MultiPathRetriever:
    """å¤šè·¯å¬å›æ£€ç´¢å™¨"""
    
    def __init__(self):
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=settings.openai_api_key,
            openai_api_base=settings.openai_api_base,
            model=settings.embedding_model
        )
        
        self.llm = ChatOpenAI(
            openai_api_key=settings.openai_api_key,
            openai_api_base=settings.openai_api_base,
            model=settings.openai_model,
            temperature=0.1
        )
        
        # è¿æ¥Milvus
        try:
            connections.connect(
                alias="default",
                host=settings.milvus_host,
                port=settings.milvus_port
            )
            self.collection = Collection(settings.milvus_collection_name)
            self.collection.load()
            print(f"âœ… Milvusè¿æ¥æˆåŠŸï¼Œcollection: {settings.milvus_collection_name}")
        except Exception as e:
            print(f"âš ï¸  Milvusè¿æ¥å¤±è´¥: {e}")
            if "not exist" in str(e):
                print(f"ğŸ’¡ æç¤ºï¼šçŸ¥è¯†åº“å°šæœªæ„å»ºï¼Œè¯·å…ˆè¿è¡Œï¼š")
                print(f"   python build_knowledge.py")
            self.collection = None
        
        # BM25ç´¢å¼•ï¼ˆç”¨äºå…³é”®è¯æ£€ç´¢ï¼‰
        self.bm25_index = None
        self.bm25_docs = []
        self._build_bm25_index()
        
        # åŒ»ç–—å…³é”®è¯è§„åˆ™åº“
        self.medical_rules = self._load_medical_rules()
    
    # ç—…ç—‡åº“ schema çš„å­—æ®µï¼ˆmedical.txt ç»“æ„ï¼‰
    MEDICAL_OUTPUT_FIELDS = ["id", "content", "name", "category_primary", "symptoms", "cure_department", "cure_way", "get_way", "cured_prob"]
    
    # ä» Milvus åˆ†æ‰¹æ‹‰å–æ—¶çš„æ¯æ‰¹æ¡æ•°
    MILVUS_QUERY_BATCH_SIZE = 2000
    
    def _build_bm25_index(self):
        """æ„å»ºBM25ç´¢å¼•ç”¨äºå…³é”®è¯æ£€ç´¢ï¼ˆä½¿ç”¨ç—…ç—‡åº“ schema å­—æ®µï¼Œåˆ†æ‰¹ä» Milvus æ‹‰å–ï¼‰"""
        if not self.collection:
            return
        
        try:
            results = []
            # ä¼˜å…ˆä½¿ç”¨ query_iterator åˆ†æ‰¹æ‹‰å–ï¼Œé¿å…å•æ¬¡ query æ•°æ®é‡è¿‡å¤§
            if hasattr(self.collection, "query_iterator"):
                it = self.collection.query_iterator(
                    batch_size=self.MILVUS_QUERY_BATCH_SIZE,
                    limit=-1,
                    expr="id != ''",
                    output_fields=self.MEDICAL_OUTPUT_FIELDS,
                )
                while True:
                    batch = it.next()
                    if not batch:
                        it.close()
                        break
                    results.extend(batch)
                    if len(batch) < self.MILVUS_QUERY_BATCH_SIZE:
                        break
            else:
                # å…¼å®¹æ—  query_iterator æ—¶ï¼šåˆ†æ‰¹ queryï¼Œç”¨ id not in æ’é™¤å·²å–
                fetched_ids = set()
                while True:
                    if fetched_ids:
                        exclude = ", ".join(f'"{x}"' for x in fetched_ids)
                        expr = f"id not in [{exclude}]"
                    else:
                        expr = "id != ''"
                    batch = self.collection.query(
                        expr=expr,
                        output_fields=self.MEDICAL_OUTPUT_FIELDS,
                        limit=self.MILVUS_QUERY_BATCH_SIZE,
                    )
                    if not batch:
                        break
                    for doc in batch:
                        fid = doc.get("id")
                        if fid and fid not in fetched_ids:
                            fetched_ids.add(fid)
                            results.append(doc)
                    if len(batch) < self.MILVUS_QUERY_BATCH_SIZE:
                        break
                    if len(results) >= 50000:  # å®‰å…¨ä¸Šé™
                        break
            
            self.bm25_docs = results

            # åˆ†è¯ï¼šå¤šè¿›ç¨‹å¹¶è¡ŒåŠ é€Ÿï¼Œæ–‡æ¡£å°‘æ—¶ç›´æ¥ç”¨ä¸»è¿›ç¨‹é¿å…è¿›ç¨‹å¼€é”€
            contents = [doc.get("content") or "" for doc in results]
            n_docs = len(contents)
            n_workers = min(max(1, cpu_count() - 1), n_docs, 8)
            if n_workers <= 1 or n_docs < 100:
                tokenized_docs = [_jieba_tokenize_one(t) for t in contents]
            else:
                with Pool(n_workers) as pool:
                    tokenized_docs = pool.map(_jieba_tokenize_one, contents, chunksize=max(1, n_docs // (n_workers * 4)))

            # æ„å»ºBM25ç´¢å¼•
            if tokenized_docs:
                self.bm25_index = BM25Okapi(tokenized_docs)
                print(f"âœ… BM25ç´¢å¼•æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(tokenized_docs)} ä¸ªæ–‡æ¡£")
        except Exception as e:
            print(f"âš ï¸  BM25ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
    
    def _load_medical_rules(self) -> Dict[str, List[str]]:
        """
        åŠ è½½åŒ»ç–—è§„åˆ™åº“
        æ ¹æ®å…³é”®è¯è§¦å‘ç‰¹å®šçš„çŸ¥è¯†æ£€ç´¢
        """
        return {
            "ç—‡çŠ¶": ["å‘çƒ§", "å’³å—½", "å¤´ç—›", "è…¹ç—›", "æ¶å¿ƒ", "å‘•å", "è…¹æ³»", "ä¹åŠ›"],
            "ç–¾ç—…": ["æ„Ÿå†’", "æµæ„Ÿ", "è‚ºç‚", "èƒƒç‚", "é«˜è¡€å‹", "ç³–å°¿ç—…", "å† å¿ƒç—…"],
            "è¯ç‰©": ["é˜¿å¸åŒ¹æ—", "å¸ƒæ´›èŠ¬", "å¯¹ä¹™é…°æ°¨åŸºé…š", "æŠ—ç”Ÿç´ ", "é™å‹è¯"],
            "æ£€æŸ¥": ["è¡€å¸¸è§„", "å°¿å¸¸è§„", "CT", "æ ¸ç£å…±æŒ¯", "Bè¶…", "Xå…‰"],
            "ç´§æ€¥": ["æ€¥æ•‘", "ä¸­æ¯’", "éª¨æŠ˜", "å‡ºè¡€", "ä¼‘å…‹", "æ˜è¿·"]
        }
    
    def vector_search(self, query: str, top_k: int = 10) -> List[KnowledgeSource]:
        """
        è·¯å¾„1ï¼šè¯­ä¹‰å‘é‡æ£€ç´¢
        ä½¿ç”¨embeddingè¿›è¡Œç›¸ä¼¼åº¦æœç´¢
        """
        if not self.collection:
            return []
        
        try:
            # å‘é‡åŒ–æŸ¥è¯¢
            query_embedding = self.embeddings.embed_query(query)
            
            # å‘é‡æœç´¢ï¼ˆç—…ç—‡åº“ schemaï¼‰
            search_params = {"metric_type": "L2", "params": {"nprobe": 10}}
            results = self.collection.search(
                data=[query_embedding],
                anns_field="embedding",
                param=search_params,
                limit=top_k,
                output_fields=self.MEDICAL_OUTPUT_FIELDS
            )
            
            # è½¬æ¢ç»“æœ
            sources = []
            for hit in results[0]:
                # Milvus L2è·ç¦»ï¼Œè¶Šå°è¶Šç›¸ä¼¼ï¼Œè½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                similarity = 1 / (1 + hit.distance)
                
                if similarity >= settings.similarity_threshold:
                    entity = hit.entity
                    content = entity.get("content") or ""
                    name = entity.get("name") or ""
                    # å±•ç¤ºæ—¶å¸¦ä¸Šç–¾ç—…åç§°
                    display = f"ã€{name}ã€‘\n{content}" if name else content
                    source = KnowledgeSource(
                        source="knowledge_base",
                        content=display,
                        score=float(similarity),
                        metadata={
                            "retrieval_type": "vector",
                            "name": name,
                            "category_primary": entity.get("category_primary"),
                            "symptoms": entity.get("symptoms"),
                            "cure_department": entity.get("cure_department"),
                            "cure_way": entity.get("cure_way"),
                            "get_way": entity.get("get_way"),
                            "cured_prob": entity.get("cured_prob"),
                        }
                    )
                    sources.append(source)
            
            print(f"ğŸ“Š å‘é‡æ£€ç´¢è¿”å› {len(sources)} æ¡ç»“æœ")
            return sources
        except Exception as e:
            print(f"âŒ å‘é‡æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def keyword_search(self, query: str, top_k: int = 10) -> List[KnowledgeSource]:
        """
        è·¯å¾„2ï¼šå…³é”®è¯/å€’æ’æ£€ç´¢ï¼ˆBM25ï¼‰
        åŸºäºè¯é¢‘å’Œé€†æ–‡æ¡£é¢‘ç‡çš„æ£€ç´¢
        """
        if not self.bm25_index or not self.bm25_docs:
            return []
        
        try:
            # åˆ†è¯
            query_tokens = list(jieba.cut(query))
            
            # BM25æ£€ç´¢
            scores = self.bm25_index.get_scores(query_tokens)
            
            # è·å–top-kç»“æœ
            top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]
            
            sources = []
            for idx in top_indices:
                score = scores[idx]
                if score > 0:  # BM25åˆ†æ•°å¤§äº0
                    doc = self.bm25_docs[idx]
                    content = doc.get("content") or ""
                    name = doc.get("name") or ""
                    display = f"ã€{name}ã€‘\n{content}" if name else content
                    source = KnowledgeSource(
                        source="knowledge_base",
                        content=display,
                        score=float(score),
                        metadata={
                            "retrieval_type": "keyword",
                            "name": name,
                            "category_primary": doc.get("category_primary"),
                            "symptoms": doc.get("symptoms"),
                            "cure_department": doc.get("cure_department"),
                            "cure_way": doc.get("cure_way"),
                            "get_way": doc.get("get_way"),
                            "cured_prob": doc.get("cured_prob"),
                        }
                    )
                    sources.append(source)
            
            print(f"ğŸ“Š å…³é”®è¯æ£€ç´¢è¿”å› {len(sources)} æ¡ç»“æœ")
            return sources
        except Exception as e:
            print(f"âŒ å…³é”®è¯æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def rule_based_search(self, query: str) -> Tuple[List[KnowledgeSource], str]:
        """
        è·¯å¾„3ï¼šè§„åˆ™å¬å›
        åŸºäºåŒ»ç–—å…³é”®è¯è§„åˆ™è§¦å‘ç‰¹å®šæ£€ç´¢
        """
        matched_category = None
        matched_keywords = []
        
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…è§„åˆ™
        for category, keywords in self.medical_rules.items():
            for keyword in keywords:
                if keyword in query:
                    matched_category = category
                    matched_keywords.append(keyword)
        
        if not matched_category:
            return [], None
        
        # å¦‚æœåŒ¹é…åˆ°ç´§æ€¥æƒ…å†µï¼Œä¼˜å…ˆè¿”å›
        if matched_category == "ç´§æ€¥":
            print(f"âš ï¸  æ£€æµ‹åˆ°ç´§æ€¥æƒ…å†µå…³é”®è¯: {matched_keywords}")
        
        # ç—…ç—‡åº“ schema æ— ã€Œç—‡çŠ¶/ç–¾ç—…/è¯ç‰©ã€ç­‰ category å­—æ®µï¼Œè§„åˆ™ä»…ä½œå…³é”®è¯åŒ¹é…ï¼Œä¸å•ç‹¬æŸ¥åº“
        # å‘é‡æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢å·²ä¼šå‘½ä¸­ç›¸å…³å†…å®¹ï¼Œè¿™é‡Œç›´æ¥è¿”å›ç©ºï¼Œé¿å…æŒ‰æ—§ schema æŸ¥åº“æŠ¥é”™
        return [], matched_category
    
    def rerank(self, query: str, sources: List[KnowledgeSource], top_k: int = 3) -> List[KnowledgeSource]:
        """
        é‡æ’ç­–ç•¥
        ä½¿ç”¨LLMå¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’ï¼Œé€‰æ‹©æœ€ç›¸å…³çš„top-k
        """
        if len(sources) <= top_k:
            return sources
        
        try:
            # æ„å»ºé‡æ’æç¤º
            candidates = "\n\n".join([
                f"[{i}] {source.content[:200]}..." 
                for i, source in enumerate(sources)
            ])
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—é—®è¯ŠåŠ©æ‰‹ã€‚ç”¨æˆ·é—®é¢˜æ˜¯ï¼š{query}

ä»¥ä¸‹æ˜¯å€™é€‰çŸ¥è¯†ç‰‡æ®µï¼š
{candidates}

è¯·æ ¹æ®ç›¸å…³æ€§å¯¹è¿™äº›çŸ¥è¯†ç‰‡æ®µæ’åºï¼Œè¿”å›æœ€ç›¸å…³çš„{top_k}ä¸ªç‰‡æ®µçš„åºå·ï¼Œç”¨é€—å·åˆ†éš”ã€‚
åªè¿”å›åºå·ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚ä¾‹å¦‚ï¼š0,3,5"""
            
            response = self.llm.invoke(prompt)
            indices_str = response.content.strip()
            
            # è§£æåºå·
            indices = [int(idx.strip()) for idx in indices_str.split(',') if idx.strip().isdigit()]
            indices = [idx for idx in indices if 0 <= idx < len(sources)][:top_k]
            
            # é‡æ’åçš„ç»“æœ
            reranked = [sources[idx] for idx in indices]
            
            print(f"ğŸ“Š é‡æ’åè¿”å› {len(reranked)} æ¡ç»“æœ")
            return reranked
        except Exception as e:
            print(f"âš ï¸  é‡æ’å¤±è´¥ï¼Œè¿”å›åŸå§‹ç»“æœ: {e}")
            # é™çº§ç­–ç•¥ï¼šæŒ‰åˆ†æ•°æ’åº
            sorted_sources = sorted(sources, key=lambda x: x.score or 0, reverse=True)
            return sorted_sources[:top_k]
    
    def retrieve(self, query: str, top_k: int = None) -> List[KnowledgeSource]:
        """
        å¤šè·¯å¬å›ä¸»å‡½æ•°
        æ•´åˆå‘é‡æ£€ç´¢ã€å…³é”®è¯æ£€ç´¢å’Œè§„åˆ™æ£€ç´¢çš„ç»“æœ
        """
        if top_k is None:
            top_k = settings.top_k_rerank
        
        print(f"ğŸ” å¼€å§‹å¤šè·¯å¬å›æ£€ç´¢ï¼Œquery: {query}")
        
        all_sources = []
        
        # è·¯å¾„1ï¼šå‘é‡æ£€ç´¢
        vector_results = self.vector_search(query, top_k=settings.top_k_retrieval)
        all_sources.extend(vector_results)
        
        # è·¯å¾„2ï¼šå…³é”®è¯æ£€ç´¢
        keyword_results = self.keyword_search(query, top_k=settings.top_k_retrieval)
        all_sources.extend(keyword_results)
        
        # è·¯å¾„3ï¼šè§„åˆ™æ£€ç´¢
        rule_results, matched_category = self.rule_based_search(query)
        all_sources.extend(rule_results)
        
        # å»é‡ï¼ˆåŸºäºå†…å®¹ï¼‰
        seen_contents = set()
        unique_sources = []
        for source in all_sources:
            content_hash = hash(source.content)
            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                unique_sources.append(source)
        
        print(f"ğŸ“Š å¤šè·¯å¬å›å…±è¿”å› {len(unique_sources)} æ¡å»é‡åçš„ç»“æœ")
        
        # é‡æ’
        if len(unique_sources) > top_k:
            final_sources = self.rerank(query, unique_sources, top_k)
        else:
            final_sources = unique_sources
        
        return final_sources
