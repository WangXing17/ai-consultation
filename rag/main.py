"""
RAGæ™ºèƒ½é—®è¯ŠåŠ©æ‰‹ - ä¸»æœåŠ¡
FastAPI + SSEæµå¼è¾“å‡º
"""
import json
import asyncio
from typing import List, AsyncGenerator
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from fastapi.staticfiles import StaticFiles
import redis
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

from config import settings
from models import ConsultRequest, ConsultResponse, KnowledgeSource, IncrementalUpdate, Document
from retriever import MultiPathRetriever
from mcp_tools import MCPToolManager
from knowledge_base import KnowledgeBase
from query_optimizer import optimize as optimize_query
from chat_history import get_messages, append_turn, messages_to_history_list


# å…¨å±€å¯¹è±¡
retriever = None
mcp_manager = None
redis_client = None
knowledge_base = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global retriever, mcp_manager, redis_client, knowledge_base
    
    print("ğŸš€ å¯åŠ¨RAGæ™ºèƒ½é—®è¯ŠåŠ©æ‰‹...")
    
    # åˆå§‹åŒ–ç»„ä»¶
    retriever = MultiPathRetriever()
    mcp_manager = MCPToolManager()
    knowledge_base = KnowledgeBase()
    
    # åˆå§‹åŒ–Redis
    try:
        redis_client = redis.Redis(
            host=settings.redis_host,
            port=settings.redis_port,
            db=settings.redis_db,
            decode_responses=True
        )
        redis_client.ping()
        print("âœ… Redisè¿æ¥æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸  Redisè¿æ¥å¤±è´¥: {e}")
        redis_client = None
    
    print("âœ… ç³»ç»Ÿå¯åŠ¨å®Œæˆ")
    
    yield
    
    # æ¸…ç†èµ„æº
    print("ğŸ‘‹ å…³é—­ç³»ç»Ÿ...")
    if redis_client:
        redis_client.close()


app = FastAPI(
    title="RAGæ™ºèƒ½é—®è¯ŠåŠ©æ‰‹",
    description="åŸºäºRAGçš„åŒ»ç–—é—®è¯Šç³»ç»Ÿï¼Œæ”¯æŒå¤šè·¯å¬å›ã€MCPå·¥å…·å…œåº•å’ŒSSEæµå¼è¾“å‡º",
    version="1.0.0",
    lifespan=lifespan
)

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æŒ‚è½½é™æ€æ–‡ä»¶ç›®å½•
try:
    app.mount("/static", StaticFiles(directory="static"), name="static")
except Exception:
    print("âš ï¸  é™æ€æ–‡ä»¶ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡æŒ‚è½½")



# LLMå®ä¾‹
llm = ChatOpenAI(
    openai_api_key=settings.openai_api_key,
    openai_api_base=settings.openai_api_base,
    model=settings.openai_model,
    temperature=0.7,
    streaming=True
)


def get_cache_key(user_id: str, question: str) -> str:
    """ç”Ÿæˆç¼“å­˜key"""
    return f"consult:{user_id}:{hash(question)}"


def check_cache(cache_key: str) -> ConsultResponse:
    """æ£€æŸ¥ç¼“å­˜"""
    if not redis_client:
        return None
    
    try:
        cached = redis_client.get(cache_key)
        if cached:
            print("ğŸ’¾ å‘½ä¸­ç¼“å­˜")
            data = json.loads(cached)
            return ConsultResponse(**data)
    except Exception as e:
        print(f"âš ï¸  ç¼“å­˜è¯»å–å¤±è´¥: {e}")
    
    return None


def set_cache(cache_key: str, response: ConsultResponse, ttl: int = 3600):
    """è®¾ç½®ç¼“å­˜"""
    if not redis_client:
        return
    
    try:
        redis_client.setex(
            cache_key,
            ttl,
            json.dumps(response.model_dump(), ensure_ascii=False)
        )
        print("ğŸ’¾ å·²ç¼“å­˜ç»“æœ")
    except Exception as e:
        print(f"âš ï¸  ç¼“å­˜å†™å…¥å¤±è´¥: {e}")


def get_request_history(request: ConsultRequest) -> List[dict]:
    """ä» Redis æŒ‰ session_id è¯»å–å¯¹è¯å†å²ï¼ˆä¸ä¾èµ–å‰ç«¯ä¼  historyï¼‰"""
    if not request.session_id or not redis_client:
        return []
    raw = get_messages(request.session_id, redis_client)
    return messages_to_history_list(raw, max_turns=6)


def build_prompt(question: str, knowledge_sources: List[KnowledgeSource], history: List) -> str:
    """æ„å»ºé—®è¯Šæç¤ºè¯ï¼ˆhistory ä¸ºæœåŠ¡ç«¯ä» Redis æ‹‰å–çš„æœ€è¿‘å‡ è½®ï¼Œæ ¼å¼ [{"role":"user"|"assistant","content":"..."}]ï¼‰"""
    
    # æ•´ç†çŸ¥è¯†æ¥æº
    knowledge_text = ""
    for i, source in enumerate(knowledge_sources, 1):
        source_type = "ã€çŸ¥è¯†åº“ã€‘" if source.source == "knowledge_base" else "ã€è”ç½‘æœç´¢ã€‘"
        knowledge_text += f"\n{source_type} æ¥æº{i}ï¼š\n{source.content}\n"
    
    # å†å²å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆè‹¥æœ‰ï¼‰
    history_block = ""
    if history:
        lines = []
        for msg in history:
            role = (msg.get("role") or "").strip()
            content = (msg.get("content") or "").strip()
            if role and content:
                lines.append(f"{'ç”¨æˆ·' if role == 'user' else 'åŠ©æ‰‹'}ï¼š{content}")
        if lines:
            history_block = "å†å²å¯¹è¯ï¼š\n" + "\n".join(lines) + "\n\n"
    
    # æ„å»ºæç¤ºè¯
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—é—®è¯ŠåŠ©æ‰‹ï¼Œå…·å¤‡ä¸°å¯Œçš„åŒ»å­¦çŸ¥è¯†ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š

1. **ç†è§£ç—…æƒ…**ï¼šä»”ç»†åˆ†æç”¨æˆ·çš„ç—‡çŠ¶æè¿°
2. **ä¿¡æ¯è¡¥å…¨**ï¼šå¦‚æœä¿¡æ¯ä¸å®Œæ•´ï¼Œä¸»åŠ¨è¯¢é—®å…³é”®ä¿¡æ¯ï¼ˆç—‡çŠ¶æŒç»­æ—¶é—´ã€ä¸¥é‡ç¨‹åº¦ã€ä¼´éšç—‡çŠ¶ç­‰ï¼‰
3. **çŸ¥è¯†æ£€ç´¢**ï¼šåŸºäºæä¾›çš„åŒ»ç–—çŸ¥è¯†ï¼Œç»™å‡ºä¸“ä¸šå»ºè®®
4. **ç»“æ„åŒ–å»ºè®®**ï¼šæä¾›æ¸…æ™°çš„åˆ†æ­¥å»ºè®®

**å›ç­”è¦æ±‚**ï¼š
- ä¸“ä¸šã€å‡†ç¡®ã€æ˜“æ‡‚
- å¼•ç”¨çŸ¥è¯†æ¥æºæ—¶æ ‡æ³¨ã€çŸ¥è¯†åº“ã€‘æˆ–ã€è”ç½‘æœç´¢ã€‘
- ç»™å‡º3-5æ¡ç»“æ„åŒ–å»ºè®®
- å¿…è¦æ—¶æé†’ç”¨æˆ·å°±åŒ»

**é‡è¦æç¤º**ï¼š
- ä½ ä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç”Ÿè¯Šæ–­
- ç´§æ€¥æƒ…å†µè¯·ç«‹å³å°±åŒ»
- å»ºè®®ä»…ä¾›å‚è€ƒ"""

    user_prompt = f"""{history_block}ç”¨æˆ·é—®é¢˜ï¼š{question}

å‚è€ƒçŸ¥è¯†ï¼š
{knowledge_text}

è¯·åŸºäºä»¥ä¸ŠçŸ¥è¯†ç»™å‡ºä¸“ä¸šçš„é—®è¯Šå»ºè®®ã€‚"""
    
    return system_prompt, user_prompt


def extract_suggestions(answer: str) -> List[str]:
    """ä»å›ç­”ä¸­æå–ç»“æ„åŒ–å»ºè®®"""
    suggestions = []
    
    # å°è¯•æå–ç¼–å·åˆ—è¡¨
    lines = answer.split('\n')
    for line in lines:
        line = line.strip()
        # åŒ¹é… "1. xxx" æˆ– "- xxx" æ ¼å¼
        if line and (line[0].isdigit() or line.startswith('-') or line.startswith('â€¢')):
            # æ¸…ç†å‰ç¼€
            suggestion = line.lstrip('0123456789.-â€¢').strip()
            if suggestion:
                suggestions.append(suggestion)
    
    return suggestions[:5]  # æœ€å¤šè¿”å›5æ¡


async def stream_response(request: ConsultRequest) -> AsyncGenerator[str, None]:
    """SSEæµå¼å“åº”ç”Ÿæˆå™¨"""
    
    try:
        # 0. ä» Redis æ‹‰å–å¯¹è¯å†å²ï¼ˆä¸ä¾èµ–å‰ç«¯ä¼  historyï¼‰
        history = get_request_history(request)

        # 1. æé—®ä¼˜åŒ–ï¼ˆä»…ç”¨äºæ£€ç´¢ï¼Œå›ç­”ä¸ç¼“å­˜ä»ç”¨åŸé—®é¢˜ï¼‰
        retrieval_query = optimize_query(
            request.question,
            history=history,
            enable_rewrite=settings.enable_query_rewrite,
            enable_normalize=settings.enable_query_normalize,
        )
        yield f"data: {json.dumps({'type': 'status', 'message': 'æ­£åœ¨æ£€ç´¢åŒ»ç–—çŸ¥è¯†...'}, ensure_ascii=False)}\n\n"
        knowledge_sources = retriever.retrieve(retrieval_query)

        # 2. MCPå·¥å…·å…œåº•
        if not knowledge_sources or (knowledge_sources and max([s.score for s in knowledge_sources if s.score], default=0) < 0.5):
            yield f"data: {json.dumps({'type': 'status', 'message': 'çŸ¥è¯†åº“ä¿¡æ¯ä¸è¶³ï¼Œæ­£åœ¨è”ç½‘æœç´¢...'}, ensure_ascii=False)}\n\n"
            knowledge_sources = await mcp_manager.enhance_retrieval(retrieval_query, knowledge_sources)
        
        # 3. å‘é€çŸ¥è¯†æ¥æº
        sources_data = [
            {
                'source': s.source,
                'content': s.content[:200] + '...' if len(s.content) > 200 else s.content,
                'score': s.score,
                'metadata': s.metadata
            }
            for s in knowledge_sources
        ]
        yield f"data: {json.dumps({'type': 'sources', 'sources': sources_data}, ensure_ascii=False)}\n\n"
        
        # 4. æ„å»ºæç¤ºè¯ï¼ˆä½¿ç”¨æœåŠ¡ç«¯å†å²ï¼‰
        system_prompt, user_prompt = build_prompt(request.question, knowledge_sources, history)
        
        # 5. æµå¼ç”Ÿæˆå›ç­”
        yield f"data: {json.dumps({'type': 'status', 'message': 'æ­£åœ¨ç”Ÿæˆå›ç­”...'}, ensure_ascii=False)}\n\n"
        
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        full_answer = ""
        async for chunk in llm.astream(messages):
            if chunk.content:
                full_answer += chunk.content
                yield f"data: {json.dumps({'type': 'content', 'content': chunk.content}, ensure_ascii=False)}\n\n"
        
        # 6. æå–å»ºè®®
        suggestions = extract_suggestions(full_answer)
        
        # 7. å‘é€å®Œæˆä¿¡å·
        yield f"data: {json.dumps({'type': 'suggestions', 'suggestions': suggestions}, ensure_ascii=False)}\n\n"
        yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
        
        # 8. ç¼“å­˜ç»“æœï¼ˆå¦‚æœæœ‰user_idï¼‰
        if request.user_id:
            response = ConsultResponse(
                answer=full_answer,
                sources=knowledge_sources,
                suggestions=suggestions
            )
            cache_key = get_cache_key(request.user_id, request.question)
            set_cache(cache_key, response)

        # 9. å°†æœ¬è½®å¯¹è¯å†™å…¥ Redisï¼ˆè‹¥æœ‰ session_idï¼‰
        if request.session_id and redis_client:
            append_turn(request.session_id, request.question, full_answer, redis_client, ttl=settings.chat_history_ttl)
    
    except Exception as e:
        error_msg = f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}"
        print(f"âŒ {error_msg}")
        yield f"data: {json.dumps({'type': 'error', 'message': error_msg}, ensure_ascii=False)}\n\n"


@app.get("/")
async def root():
    """å¥åº·æ£€æŸ¥"""
    return {
        "service": "RAGæ™ºèƒ½é—®è¯ŠåŠ©æ‰‹",
        "status": "running",
        "version": "1.0.0"
    }


@app.post("/api/consult/stream")
async def consult_stream(request: ConsultRequest):
    """
    é—®è¯Šæ¥å£ï¼ˆSSEæµå¼ï¼‰
    """
    # æ£€æŸ¥ç¼“å­˜
    if request.user_id:
        cache_key = get_cache_key(request.user_id, request.question)
        cached_response = check_cache(cache_key)
        if cached_response:
            # è¿”å›ç¼“å­˜çš„å®Œæ•´å“åº”
            async def cached_stream():
                yield f"data: {json.dumps({'type': 'cached', 'message': 'ä½¿ç”¨ç¼“å­˜ç»“æœ'}, ensure_ascii=False)}\n\n"
                yield f"data: {json.dumps({'type': 'content', 'content': cached_response.answer}, ensure_ascii=False)}\n\n"
                yield f"data: {json.dumps({'type': 'suggestions', 'suggestions': cached_response.suggestions}, ensure_ascii=False)}\n\n"
                yield f"data: {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"
            
            return StreamingResponse(
                cached_stream(),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                }
            )
    
    # æµå¼å“åº”
    return StreamingResponse(
        stream_response(request),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


@app.post("/api/consult", response_model=ConsultResponse)
async def consult(request: ConsultRequest):
    """
    é—®è¯Šæ¥å£ï¼ˆéæµå¼ï¼‰
    """
    # æ£€æŸ¥ç¼“å­˜
    if request.user_id:
        cache_key = get_cache_key(request.user_id, request.question)
        cached_response = check_cache(cache_key)
        if cached_response:
            return cached_response
    
    try:
        # 0. ä» Redis æ‹‰å–å¯¹è¯å†å²
        history = get_request_history(request)

        # 1. æé—®ä¼˜åŒ–ï¼ˆä»…ç”¨äºæ£€ç´¢ï¼‰
        retrieval_query = optimize_query(
            request.question,
            history=history,
            enable_rewrite=settings.enable_query_rewrite,
            enable_normalize=settings.enable_query_normalize,
        )
        knowledge_sources = retriever.retrieve(retrieval_query)

        # 2. MCPå·¥å…·å…œåº•
        knowledge_sources = await mcp_manager.enhance_retrieval(retrieval_query, knowledge_sources)

        # 3. æ„å»ºæç¤ºè¯ï¼ˆä»ç”¨åŸå§‹é—®é¢˜ï¼Œå†å²æ¥è‡ª Redisï¼‰
        system_prompt, user_prompt = build_prompt(request.question, knowledge_sources, history)
        
        # 4. ç”Ÿæˆå›ç­”
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        response = await llm.ainvoke(messages)
        answer = response.content
        
        # 5. æå–å»ºè®®
        suggestions = extract_suggestions(answer)
        
        # 6. æ„å»ºå“åº”
        result = ConsultResponse(
            answer=answer,
            sources=knowledge_sources,
            suggestions=suggestions
        )
        
        # 7. ç¼“å­˜ç»“æœ
        if request.user_id:
            set_cache(cache_key, result)

        # 8. å°†æœ¬è½®å¯¹è¯å†™å…¥ Redisï¼ˆè‹¥æœ‰ session_idï¼‰
        if request.session_id and redis_client:
            append_turn(request.session_id, request.question, answer, redis_client, ttl=settings.chat_history_ttl)
        
        return result
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é—®è¯Šå¤±è´¥: {str(e)}")


@app.post("/api/knowledge/build")
async def build_knowledge_base(file_path: str):
    """
    æ„å»ºçŸ¥è¯†åº“ï¼ˆæ—§ç‰ˆ JSON æ ¼å¼ï¼Œå¦‚ data/medical_knowledge.jsonï¼‰
    """
    try:
        knowledge_base.build_knowledge_base(file_path)
        
        # é‡å»ºBM25ç´¢å¼•
        retriever._build_bm25_index()
        
        return {"message": "çŸ¥è¯†åº“æ„å»ºæˆåŠŸ", "file": file_path}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ„å»ºå¤±è´¥: {str(e)}")


@app.post("/api/knowledge/build_medical")
async def build_medical_knowledge(file_path: str = "data/medical.txt"):
    """
    ä» medical.txtï¼ˆJSONL ç—…ç—‡æ•°æ®ï¼‰æ„å»ºç—…ç—‡åº“ã€‚
    ä¼šå…ˆåˆ é™¤åŒå collection å†æŒ‰æ–° schema åˆ›å»ºå¹¶å†™å…¥æ•°æ®ã€‚
    """
    try:
        knowledge_base.build_medical_knowledge_base(file_path)
        
        # åˆ·æ–°æ£€ç´¢å™¨ä½¿ç”¨çš„ collection å¹¶é‡å»º BM25
        from pymilvus import Collection
        retriever.collection = Collection(settings.milvus_collection_name)
        retriever.collection.load()
        retriever._build_bm25_index()
        
        return {"message": "ç—…ç—‡åº“æ„å»ºæˆåŠŸ", "file": file_path}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ„å»ºå¤±è´¥: {str(e)}")


@app.post("/api/knowledge/update")
async def update_knowledge_base(request: IncrementalUpdate):
    """
    å¢é‡æ›´æ–°çŸ¥è¯†åº“
    """
    try:
        knowledge_base.incremental_update(request.documents, request.update_type)
        
        # é‡å»ºBM25ç´¢å¼•
        retriever._build_bm25_index()
        
        return {
            "message": "å¢é‡æ›´æ–°æˆåŠŸ",
            "update_type": request.update_type,
            "count": len(request.documents)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–°å¤±è´¥: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
